{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2cde0a",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ca3d6",
   "metadata": {},
   "source": [
    "### What is Decision Tree and why do we need one?\n",
    "\n",
    "Decision tree is a non-parametric supervised machine learning algorithm which is used mostly for classification and regression problems.If the relationship between dependent and independent variable is well approximated by a linear model, linear regression will outperform tree-based model but if there is high non-linearity and complex relationship between dependent and independent variables, a tree-based model will outperform a classical regression method. If we need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression.\n",
    "\n",
    "### Types of Decision Tree\n",
    "\n",
    "There are two types of decision trees that are based on the *target variable*, i.e., categorical variable decision tree and continuous variable decision tree.\n",
    "\n",
    "+ **Categorical Variable Decision Tree**: A categorical variable decision tree includes categorical target variables that are divided into two categories. For example, the categories can be ‘Yes’ or ‘No’. The categories mean that every stage of the decision process falls into one of the categories and there are no in-betweens.\n",
    "\n",
    "+ **Continuous Variable Decision Tree**: A continuous variable decision tree is a decision tree with a continuous target variable. For example, the income of an individual whose income is unknown can be predicted based on the available information such as their occupation, age and other continuous variables.\n",
    "\n",
    "\n",
    "### Important terminology related to Decision Tree\n",
    "\n",
    "- ***Root Node***: It represents the entire population or sample and this further gets into two or more homogeneous sets.\n",
    "- ***Splitting***: It is a process of dividing a node into two or more sub nodes.\n",
    "- ***Decision Node***: When a sub-node splits into further sub nodes, then it is called the decision tree.\n",
    "- ***Leaf/Terminal Node***: Nodes do not split is called leaf or terminal node.\n",
    "- ***Pruning***: When we remove sub-nodes of a decision node, this process is called pruning.\n",
    "- ***Branch/Sub-Tree***: A subsection of the entire tree is called branch/sub tree.\n",
    "- ***Parent and Child Node***: A node, which is divided into sub nodes is called a parent node of sub nodes whereas sub nodes are the child of a parent node.\n",
    "\n",
    "### How does a decision tree handle numerical and categorical data?\n",
    "\n",
    "Decision tree can handle both numerical and categorical variables at the same time as features. There is not any problem in doing that. Every split in a decision tree is based on a feature.\n",
    "\n",
    "+ If the feature is categorical, the split is done with the elements belonging to a particular class.\n",
    "+ If the feature is continuous, the split is done with the elements higher than a threshold.\n",
    "\n",
    "At every split, the decision tree will take the best variable at that moment. This will be done according to an impurity measure with the split branches. And the fact that, the variable used to do split is categorical or continuous is irrelevant.\n",
    "\n",
    "*At last, the good approach is to always convert the categorical to continuous One-hot Encoding.*\n",
    "\n",
    "\n",
    "### How to choose the set of split points\n",
    "\n",
    "The set of split points considered for any variable depends upon whether the variable is numeric or categorical.\n",
    "\n",
    "**When a predictor is numeric** and if all values are unique then there are *n – 1* split points for *n* data points. Because this may be a large number, it is common to consider only split points at certain percentiles of the distribution of values. For example, we may consider every tenth percentile (that is, 10%, 20%, 30%, etc).\n",
    "\n",
    "Example: If the predictor variable column is [20, 25, 30, 35, 50, 55, 70, 90]. We have 8 data points. Then for split points, average between each set of data point is considered which will give us 7 values [22.5, 27.5, 32.5, 42.5, 52.5, 62.5, 80.0]. Then we calculate Information gain (IG) considering each split point and choose the one which gives max IG.\n",
    "    \n",
    "\n",
    "**When a predictor is categorical** we can decide to split it to create either one child node per class (*multiway splits*) or only two child nodes (*binary split*). It is usual to make only binary splits because multiway splits break the data into small subsets too quickly. This causes a bias towards splitting predictors with many classes since they are more likely to produce relatively pure child nodes, which results in overfitting.\n",
    "\n",
    "If a categorical predictor has only two classes, there is only one possible split. However, if a categorical predictor has more than two classes, various conditions can apply. If there is a small number of classes, all possible splits into two child nodes can be considered. For example, for classes apple, banana and orange the three splits are as shown below.\n",
    "\n",
    "|        | child1 |     child2     |\n",
    "|:------:|:------:|:--------------:|\n",
    "| split1 |  apple | banana, orange |\n",
    "| split2 | banana |  apple, orange |\n",
    "| split3 | orange |  apple, banana |\n",
    "\n",
    "For k classes there are $2^{k – 1} – 1$ splits, which is computationally prohibitive if k is a large number.    \n",
    "    \n",
    "If there are many classes, they may be ordered according to their average output value. We can the make a binary split into two groups of the ordered classes. This means there are k – 1 possible splits for k classes. If k is large, there are more splits to consider. As a result, there is a greater chance that a certain split will create a significant improvement, and is therefore best. This causes trees to be biased towards splitting variables with many classes over those with fewer classes.\n",
    "\n",
    "\n",
    "###  Criteria used to imporove the split\n",
    "\n",
    "The type of criteria used while splittig the node depends on our target variable wether it is continous or categorical in nature.\n",
    "\n",
    "**Continuous Target Variable**\n",
    "+ **Reduction in sum of Squared Errors (SSE)**\n",
    "\n",
    "When the outcome is numeric, the relevant improvement is the difference in the sum of squared errors between the node and its child nodes after the split. For any node, the squared error is:\n",
    "\n",
    "  $$ \\sum_{i=1}^{n}{(y_i-c)}^2 $$\n",
    "  \n",
    "where n is the number of cases at that node, c is the average outcome of all cases at that node, and yi is the outcome value of the ith case. If all the yi are close to c, then the error is low. A good clean split will create two nodes which both have all case outcomes close to the average outcome of all cases at that node.\n",
    "\n",
    "\n",
    "**Categorical Target Variable**\n",
    "+ **Gini Impurity**  \n",
    "\n",
    "    When the outcome is categorical, the split may be based on either the improvement of Gini impurity or cross-entropy:\n",
    "\n",
    "  $$ Gini\\ impurity=\\sum_{i=1}^{k}{p_i(1-p_i)}$$ \n",
    "  \n",
    "    where $k$ is the number of classes and $p_i$ is the proportion of cases belonging to class $i$. These two measures give similar results and are minimal when the probability of class membership is close to zero or one. The goal is to split with the largest reduction in weighted gini impurity from child nodes from gini impurity of parent node.\n",
    "\n",
    "\n",
    "+ **Information Gain** \n",
    "\n",
    "    Entropy : In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. It is calculated by the formula.\n",
    "    $$Cross\\ entropy=-\\sum_{i=1}^{k}{p_ilog_2(p_i)} $$\n",
    "    \n",
    "    If the entropy is high it is more difficult to predict the outcomes and vice versa. Consider an example if flipping a coin. It has 2 outcomes heads and tails. Now lets calcualte the entropy.\n",
    "    \n",
    "    probability of head or tail  $1/2$\n",
    "    \n",
    "    $$H = - \\bigg ( p_{head}(log_2\\ p_{head}) + p_{tail}(log_2\\ p_{tail})\\bigg) = - \\bigg ( \\frac{1}{2}(log_2\\ \\frac{1}{2}) + \\frac{1}{2}(log_2\\ \\frac{1}{2})\\bigg) =  -2 *\\bigg ( \\frac{1}{2}\\bigg )log_2\\ \\frac{1}{2} = 1 $$\n",
    "    \n",
    "    If we consider rolla a die then the probabilty of getting any particular outcome from 1 to 6 is $1/6$. So the entopy will be\n",
    "    \n",
    "    $$ H = -6 *\\bigg ( \\frac{1}{6}\\bigg )log_2\\ \\frac{1}{6} = 2.58 $$\n",
    "    \n",
    "    We notice that the entropy of rolling a die is higher so it can be said that predicting the outcoem of dice is moch difficult than of the outcome of predicting a coin toss.\n",
    "    \n",
    "    When multiple variables are involved one way to understand the importance of variable in predicting results is by estimating *Information gain* using that variable. Information gain is calculated by diffrence in entropy before split and after split. \n",
    "    \n",
    "    $$ Information\\ Gain = Entropy(Parent) - Entropy(Children) $$\n",
    "    \n",
    "    we will look more on how this is calculated in the coming sections in detail.\n",
    "    \n",
    "    \n",
    "+ **Chi-Square**\n",
    "\n",
    "    Chi-square is another method of splitting nodes in a decision tree for datasets having categorical target values. It can make two or more than two splits. It works on the statistical significance of differences between the parent node and child nodes.\n",
    "\n",
    "    Chi-Square value is:\n",
    "\n",
    "    $$Chi-Square = \\sqrt{\\frac{(Actual-Expected)^2}{Expected}}$$\n",
    "\n",
    "    Here, the Expected is the expected value for a class in a child node based on the distribution of classes in the parent node, and Actual is the actual value for a class in a child node.\n",
    "\n",
    "    The above formula gives us the value of Chi-Square for a class. Take the sum of Chi-Square values for all the classes in a node to calculate the Chi-Square for that node. Higher the value, higher will be the differences between parent and child nodes, i.e., higher will be the homogeneity.\n",
    "\n",
    "\n",
    "### Illustration of node splitting considering Information gain for a Decision Tree\n",
    "\n",
    "Consider an Experiment in which we have two predictors *variable1* and *variable2* and Target has two outcomes *stop* and *continue*.\n",
    "\n",
    "| variable1 | variable2 |  outcome |\n",
    "|:---------:|:---------:|:--------:|\n",
    "|     3     |     5     |   stop   |\n",
    "|     7     |     6     | continue |\n",
    "|     3     |     3     |   stop   |\n",
    "|     4     |     8     | continue |\n",
    "|     3     |     9     | continue |\n",
    "|     6     |     5     |   stop   |\n",
    "|     5     |     8     | continue |\n",
    "|     6     |     4     | continue |\n",
    "\n",
    "Now lets calculate the intial entropy of root node before any spliting\n",
    "\n",
    "$$H = -\\bigg (\\frac{3}{8}\\bigg )log_2\\frac{3}{8} - \\bigg (\\frac{5}{8}\\bigg )log_2\\frac{5}{8} = 0.954$$\n",
    "\n",
    "Now we have two options either to choose variable1 for split or variable2. To decide on which variable to splot on  we will first calculate entropies in both the cases\n",
    "\n",
    "If we choose *variable1* for split on split point 4 (which is average or 50 precentile) then we calulate *weighted entropy* after the split as follows\n",
    "\n",
    "$p_{(>4)} = 4/8$\n",
    "\n",
    "\n",
    "$H_{(>4)} = -\\frac{1}{4}log_2\\frac{1}{4} - \\frac{3}{4}log_2\\frac{3}{4} = 0.81$\n",
    "\n",
    "| variable1 | outcome |\n",
    "|:---------:|:--------:|\n",
    "|     7     | continue |\n",
    "|     6     |   stop   |\n",
    "|     5     | continue |\n",
    "|     6     | continue |\n",
    "\n",
    "$p_{(<=4)} = 4/8$\n",
    "\n",
    "\n",
    "$H_{(<=4)} = -\\frac{2}{4}log_2\\frac{2}{4} - \\frac{2}{4}log_2\\frac{2}{4} = 1.0$\n",
    "\n",
    "| variable1 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     3     |   stop   |\n",
    "|     3     |   stop   |\n",
    "|     4     | continue |\n",
    "|     3     | continue |\n",
    "\n",
    "Entropy after splitting by *variable1* is $H(variable1) = -p_{(>4)}H_{(>4)} - p_{(<=4)}H_{(<=4)} = 0.9$\n",
    "\n",
    "we choose *variable2* for split on split point 6 (which is average or 50 precentile) then we calulate *weighted entropy* after the split as follows\n",
    "\n",
    "$p_{(>6)} = 3/8$\n",
    "\n",
    "$H_{(>6)} = -\\frac{3}{3}log_2\\frac{3}{3}-\\frac{0}{3}log_2\\frac{0}{3} = 0$\n",
    "\n",
    "| variable2 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     8     | continue |\n",
    "|     9     | continue |\n",
    "|     8     | continue |\n",
    "\n",
    "\n",
    "$p_{(<=6)} = 3/8$\n",
    "\n",
    "$H_{(<=6)} = -\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5} = 0.97$\n",
    "\n",
    "| variable2 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     5     |   stop   |\n",
    "|     6     | continue |\n",
    "|     3     |   stop   |\n",
    "|     5     |   stop   |\n",
    "|     4     | continue |\n",
    "\n",
    "Entropy after splitting by *variable2* is $H(variable2) = - p_{(>6)}H_{(>6)} - p_{(<=6)}H_{(<=6)} = 0.28$\n",
    "\n",
    "Now let us calculate information gain after each split considering *variable1* and *variable2*.\n",
    "\n",
    "$$IG(1) = H - H(variable1) = 0.954 - 0.9 = 0.054$$\n",
    "\n",
    "$$IG(2) = H - H(variable2) = 0.954 - 0.28 = 0.674$$\n",
    "\n",
    "We see that if we split the node by consedering *variable2* we have *highest information gain*. So we select *variable2* for the split and then we continue repeating the process for impure nodes till we obtain pure leaf nodes for the decision tree. Following diagram shows the decision tree in this case.\n",
    "\n",
    "![](./fig/DecisionTree.png)\n",
    "\n",
    "### Illustration of node splitting considering Gini Impurity for a Decision Tree\n",
    "\n",
    "Consider the same Experiment data from above.\n",
    "\n",
    "| variable1 | variable2 |  outcome |\n",
    "|:---------:|:---------:|:--------:|\n",
    "|     3     |     5     |   stop   |\n",
    "|     7     |     6     | continue |\n",
    "|     3     |     3     |   stop   |\n",
    "|     4     |     8     | continue |\n",
    "|     3     |     9     | continue |\n",
    "|     6     |     5     |   stop   |\n",
    "|     5     |     8     | continue |\n",
    "|     6     |     4     | continue |\n",
    "\n",
    "We have two variables for splitting. we have to calculate weighted gini impurity for both vairable1 and variable2. Whichever variable gives the lowest impurity we will select that variable for split.\n",
    "\n",
    "Lets take the split point as 4 (average value) and calcuate the impurity for variable1\n",
    "\n",
    "$$Gini Impurity (>4) = 1 - \\bigg(\\bigg(\\frac{3}{4}\\bigg)^2 + (\\bigg(\\frac{1}{4}\\bigg)^2\\bigg) = 0.375$$\n",
    "\n",
    "| variable1 | outcome |\n",
    "|:---------:|:--------:|\n",
    "|     7     | continue |\n",
    "|     6     |   stop   |\n",
    "|     5     | continue |\n",
    "|     6     | continue |\n",
    "\n",
    "$$Gini Impurity (<=4) = 1 - \\bigg(\\bigg(\\frac{2}{4}\\bigg)^2 + (\\bigg(\\frac{2}{4}\\bigg)^2\\bigg) = 0.375$$\n",
    "\n",
    "| variable1 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     3     |   stop   |\n",
    "|     3     |   stop   |\n",
    "|     4     | continue |\n",
    "|     3     | continue |\n",
    "\n",
    "$$Weighted Gini Impurity(variable1) = \\frac{4}{8}*0.375 +  \\frac{4}{8}*0.5 = 0.4375$$\n",
    "\n",
    "Lets take the split point as 6 (average value) and calcuate the impurity for variable2\n",
    "\n",
    "$$Gini Impurity (>6) = 1 - \\bigg(\\bigg(\\frac{3}{3}\\bigg)^2 + (\\bigg(\\frac{0}{3}\\bigg)^2\\bigg) = 0$$\n",
    "\n",
    "| variable2 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     8     | continue |\n",
    "|     9     | continue |\n",
    "|     8     | continue |\n",
    "\n",
    "$$Gini Impurity (<=6) = 1 - \\bigg(\\bigg(\\frac{2}{5}\\bigg)^2 + (\\bigg(\\frac{3}{5}\\bigg)^2\\bigg) = 0.48$$\n",
    "\n",
    "| variable2 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     5     |   stop   |\n",
    "|     6     | continue |\n",
    "|     3     |   stop   |\n",
    "|     5     |   stop   |\n",
    "|     4     | continue |\n",
    "\n",
    "$$Weighted Gini Impurity(variable2) = \\frac{3}{8}*0 +  \\frac{5}{8}*0.48= 0.3$$\n",
    "\n",
    "From the above calculations we see that Weighted gini impurity is less for variable2. So we choose variable 2 for splitting the data. This process is continued recursively to build the entire decision tree.\n",
    "\n",
    "**Note**: For same data using gini impurity and entropy measure, different variables can be selected as root node, THis is because of bias of each algorithm. The gini impurity is biased toawards selecting wider spread of data for variable selection, whereas the entopy method is biased toward compact data with lower spread.\n",
    "\n",
    "### Illustration of node splitting considering Chi-Square for a Decision Tree\n",
    "\n",
    "Lets look at another example which has categorical predictor and categorical target for this for ease of understanding\n",
    "\n",
    "|Performance| Class |    Outcome   |\n",
    "|:---------:|:-----:|:------------:|\n",
    "|  Average  |   IX  | Play Cricket |\n",
    "| Below Avg |   X   | Play Cricket |\n",
    "|  Average  |   IX  | Play Cricket |\n",
    "| Below Avg |   X   | Play Cricket |\n",
    "| Below Avg |   X   | Play Cricket |\n",
    "|  Average  |   IX  | Play Cricket |\n",
    "|  Average  |   X   | Doesn't Play |\n",
    "|  Average  |   IX  | Doesn't Play |\n",
    "| Below Avg |   X   | Doesn't Play |\n",
    "|  Average  |   IX  | Doesn't Play |\n",
    "|  Average  |   X   | Doesn't Play |\n",
    "| Below Avg |   IX  | Doesn't Play |\n",
    "\n",
    "\n",
    "Distribution of target before splitting (Parent node):\n",
    "\n",
    "+ play's cricket = $6/12 = 0.5$  \n",
    "+ doesnt play cricket = $6/12 = 0.5$\n",
    "\n",
    "Lets try to split the node by considering preformance variable and calculate the chisquare.\n",
    "\n",
    "\n",
    "|Performance|    Outcome   |\n",
    "|:---------:|:------------:|\n",
    "|  Average  | Play Cricket |\n",
    "|  Average  | Play Cricket |\n",
    "|  Average  | Play Cricket |\n",
    "|  Average  | Play Cricket |\n",
    "|  Average  | Doesn't Play |\n",
    "|  Average  | Doesn't Play |\n",
    "|  Average  | Doesn't Play |\n",
    "\n",
    "\n",
    "Performance: Average = $7$  \n",
    "Expected cricket players = $7 *(0.5) = 3.5$  \n",
    "Actual Cricket players = $4$  \n",
    "Chi-square Players = $0.267$  \n",
    "  \n",
    "Expected non players = $7*(0.5) = 3.5$  \n",
    "Actual non players = $3$  \n",
    "Chi-square non players = $0.267$  \n",
    "  \n",
    "|Performance|    Outcome   |\n",
    "|:---------:|:------------:|\n",
    "| Below Avg | Play Cricket |\n",
    "| Below Avg | Play Cricket |\n",
    "| Below Avg | Play Cricket |\n",
    "| Below Avg | Doesn't Play |\n",
    "| Below Avg | Doesn't Play |\n",
    "  \n",
    "Performance: Below Avg = $5$  \n",
    "Expected cricket players = $5 *(0.5) = 2.5$  \n",
    "Actual Cricket players = $3$  \n",
    "Chi-square Players = $0.316$  \n",
    "  \n",
    "Expected non players = $5*(0.5) = 2.5$  \n",
    "Actual non players = $2$  \n",
    "Chi-square non players = $0.316$  \n",
    "  \n",
    "**Total Chi-Square(Performance)** = $0.316+0.316+0.267+0.267$ = $1.166$  \n",
    "  \n",
    "Lets try to split the node by considering preformance variable and calculate the chisquare.\n",
    "\n",
    "\n",
    "  \n",
    "| Class |    Outcome   |\n",
    "|:-----:|:------------:|\n",
    "|   IX  | Play Cricket |\n",
    "|   IX  | Play Cricket |\n",
    "|   IX  | Play Cricket |\n",
    "|   IX  | Doesn't Play |\n",
    "|   IX  | Doesn't Play |\n",
    "|   IX  | Doesn't Play |\n",
    "  \n",
    "Class: IX = $6$  \n",
    "Expected cricket players = $6*(0.5) = 3$  \n",
    "Actual Cricket players = $3$  \n",
    "Chi-square Players = $0$  \n",
    "  \n",
    "Expected non players = $6*(0.5) = 3$  \n",
    "Actual non players = $3$  \n",
    "Chi-square non Players = $0$  \n",
    "  \n",
    "| Class |    Outcome   |\n",
    "|:-----:|:------------:|\n",
    "|   X   | Play Cricket |\n",
    "|   X   | Play Cricket |\n",
    "|   X   | Play Cricket |\n",
    "|   X   | Doesn't Play |\n",
    "|   X   | Doesn't Play |\n",
    "|   X   | Doesn't Play |\n",
    "  \n",
    "Class: X = $6$  \n",
    "Expected cricket players = $6*(0.5) = 3$  \n",
    "Actual Cricket players = $3$  \n",
    "Chi-square Players = $0$  \n",
    "  \n",
    "Expected non players = $6*(0.5) = 3$  \n",
    "Actual non players = $3$  \n",
    "Chi-square non Players = $0$  \n",
    "  \n",
    "**Total Chi-Square(class)** = $0+0+0+0$ = $0$\n",
    "\n",
    "We see Total chi-square is greater in case of splitting by performance than class so we choose performance to split the root node. This is repeated recursively till we obtain pure leaf nodes.\n",
    "\n",
    "### Advantages of Decision Tree\n",
    "\n",
    "+ It can be used for both classification and regression problems.\n",
    "+ It can handle both continuous and categorical variables.\n",
    "+ No feature scaling (standardization and normalization) required in case of decision tree as it uses rule-based approach instead of distance calculation.\n",
    "+ Non linear parameters don't affect the performance of a decision tree unlike the curve-based algorithms. So, there is high non-linearity between the independent variables, Decision tree may out form as compared to the other curve-based algorithms. \n",
    "+ Decision tree can automatically handle missing values.\n",
    "+ Decision tree is usually robust to outlier and can handle them automatically.\n",
    "+ Training period is less as compared to Random Forest because it generates only one tree unlike forest of trees in the Random Forest.\n",
    "\n",
    "### Disadvantages of Decision Tree\n",
    "\n",
    "+ Overfitting is the main problem of decision tree. It generally leads to overfitting of the data which ultimately leads to wrong prediction.\n",
    "+ Due to the overfitting, there are very high chances of high variance in the output which leads to many errors in the final estimation and shows high in accuracy in the results.\n",
    "+ Adding a new datapoint can lead to re-generation of the overall tree and all nodes need to be recalculated and recreated.\n",
    "+ Decision tree is not suitable for large datasets. If the dataset is large, then one single tree may grow complex and lead to overfit. So, in this case, we should use random forest instead of a single decision tree.\n",
    "\n",
    "### Popular Algorithms\n",
    "\n",
    "Some of the popular algorithms used for constructing decision trees are:\n",
    "\n",
    "1. ID3 (Iterative Dichotomiser): Uses Information Gain as attribute selection measure.\n",
    "\n",
    "2. C4.5 (Successor of ID3):  Uses Gain Ratio as attribute selection measure.\n",
    "\n",
    "3. CART (Classification and Regression Trees) – Uses Gini Index as attribute selection measure.\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "There are several methods used by various decision trees. Simply ignoring the missing values (like ID3 and other old algorithms does) or treating the missing values as another category (in case of a nominal feature) are not real handling missing values. However those approaches were used in the early stages of decision tree development.\n",
    "\n",
    "The real handling approaches to missing data does not use data point with missing values in the evaluation of a split. However, when child nodes are created and trained, those instances are distributed somehow.\n",
    "\n",
    "Approaches to distribute the missing value instances to child nodes:\n",
    "\n",
    "+ All goes to the node which already has the biggest number of instances (CART, is not the primary rule) distribute to all children, but with diminished weights, proportional with the number of instances from each child node (C45 and others)\n",
    "+ distribute randomly to only one single child node, eventually according with a categorical distribution (I have seen that in various implementations of C45 and CART for a faster running time)\n",
    "+ build, sort and use surrogates to distribute instances to a child node, where surrogates are input features which resembles best how the test feature send data instances to left or right child node (CART, if that fails, the majority rule is used)\n",
    "+ We choose the missing calue to be either of the values (0 or 1) in a way which minimizes the classification error.\n",
    "\n",
    "### Robust to Outliers?\n",
    "\n",
    "+ Regression Tree depends on average square values of both groups. So It will be effected by the presence of outliers\n",
    "+ Classification tree will not be affected by outliers.\n",
    "\n",
    "### Pruning a Tree\n",
    "\n",
    "As the name implies, pruning involves cutting back the tree. After a tree has been built (and in the absence of early stopping discussed below) it may be overfitted. The CART algorithm will repeatedly partition data into smaller and smaller subsets until those final subsets are homogeneous in terms of the outcome variable. In practice this often means that the final subsets (known as the leaves of the tree) each consist of only one or a few data points. The tree has learned the data exactly, but a new data point that differs very slightly might not be predicted well.\n",
    "\n",
    "**Post Pruning**\n",
    "\n",
    "+ **Minimum error**. The tree is pruned back to the point where the cross-validated error is a minimum. Cross-validation is the process of building a tree with most of the data and then using the remaining part of the data to test the accuracy of the decision tree.\n",
    "+ **Smallest tree**. The tree is pruned back slightly further than the minimum error. Technically the pruning creates a decision tree with cross-validation error within 1 standard error of the minimum error. The smaller tree is more intelligible at the cost of a small increase in error.\n",
    "\n",
    "**Pre Pruning**\n",
    "\n",
    "An alternative method to prevent overfitting is to try and stop the tree-building process early, before it produces leaves with very small samples. This heuristic is known as early stopping but is also sometimes known as pre-pruning decision trees.\n",
    "At each stage of splitting the tree, we check the cross-validation error. If the error does not decrease significantly enough then we stop. Early stopping may underfit by stopping too early. The current split may be of little benefit, but having made it, subsequent splits more significantly reduce the error. Early stopping and pruning can be used together, separately, or not at all.  Post pruning decision trees is more mathematically rigorous, finding a tree at least as good as early stopping. Early stopping is a quick fix heuristic.\n",
    "\n",
    "If used together with pruning, early stopping may save time. After all, why build a tree only to prune it back again?\n",
    "\n",
    "For best accuracy, minimum error pruning without early stopping is usually a good choice.\n",
    "For a compromise between accuracy and an interpretable tree, try smallest tree pruning without early stopping.\n",
    "To produce an even smaller tree or reduce the running time while allowing accuracy to decrease,  you can turn on early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e5fcb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
