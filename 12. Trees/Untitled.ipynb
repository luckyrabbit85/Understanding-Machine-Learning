{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2cde0a",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ca3d6",
   "metadata": {},
   "source": [
    "### What is Decision Tree and why do we need one?\n",
    "\n",
    "Decision tree is a non-parametric supervised machine learning algorithm which is used mostly for classification and regression problems.If the relationship between dependent and independent variable is well approximated by a linear model, linear regression will outperform tree-based model but if there is high non-linearity and complex relationship between dependent and independent variables, a tree-based model will outperform a classical regression method. If we need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression.\n",
    "\n",
    "### Types of Decision Tree\n",
    "\n",
    "There are two types of decision trees that are based on the *target variable*, i.e., categorical variable decision tree and continuous variable decision tree.\n",
    "\n",
    "+ **Categorical Variable Decision Tree**: A categorical variable decision tree includes categorical target variables that are divided into two categories. For example, the categories can be ‘Yes’ or ‘No’. The categories mean that every stage of the decision process falls into one of the categories and there are no in-betweens.\n",
    "\n",
    "+ **Continuous Variable Decision Tree**: A continuous variable decision tree is a decision tree with a continuous target variable. For example, the income of an individual whose income is unknown can be predicted based on the available information such as their occupation, age and other continuous variables.\n",
    "\n",
    "\n",
    "### Important terminology related to Decision Tree\n",
    "\n",
    "- ***Root Node***: It represents the entire population or sample and this further gets into two or more homogeneous sets.\n",
    "- ***Splitting***: It is a process of dividing a node into two or more sub nodes.\n",
    "- ***Decision Node***: When a sub-node splits into further sub nodes, then it is called the decision tree.\n",
    "- ***Leaf/Terminal Node***: Nodes do not split is called leaf or terminal node.\n",
    "- ***Pruning***: When we remove sub-nodes of a decision node, this process is called pruning.\n",
    "- ***Branch/Sub-Tree***: A subsection of the entire tree is called branch/sub tree.\n",
    "- ***Parent and Child Node***: A node, which is divided into sub nodes is called a parent node of sub nodes whereas sub nodes are the child of a parent node.\n",
    "\n",
    "### How to handle a decision tree for numerical and categorical data?\n",
    "\n",
    "Decision tree can handle both numerical and categorical variables at the same time as features. There is not any problem in doing that. Every split in a decision tree is based on a feature.\n",
    "\n",
    "+ If the feature is categorical, the split is done with the elements belonging to a particular class.\n",
    "+ If the feature is continuous, the split is done with the elements higher than a threshold.\n",
    "\n",
    "At every split, the decision tree will take the best variable at that moment. This will be done according to an impurity measure with the split branches. And the fact that, the variable used to do split is categorical or continuous is irrelevant.\n",
    "\n",
    "*At last, the good approach is to always convert the categorical to continuous One-hot Encoding.*\n",
    "\n",
    "\n",
    "### Choosing the set of split points to test\n",
    "\n",
    "The set of split points considered for any variable depends upon whether the variable is numeric or categorical.\n",
    "\n",
    "+ **When a predictor is numeric** and if all values are unique then there are *n – 1* split points for *n* data points. Because this may be a large number, it is common to consider only split points at certain percentiles of the distribution of values. For example, we may consider every tenth percentile (that is, 10%, 20%, 30%, etc).\n",
    "\n",
    "    Example: If the predictor variable column is [20, 25, 30, 35, 50, 55, 70, 90]. We have 8 data points. Then for split points, average between each set of data point is considered which will give us 7 values [22.5, 27.5, 32.5, 42.5, 52.5, 62.5, 80.0]. Then we calculate Information gain (IG) considering each split point and choose the one which gives max IG.\n",
    "    \n",
    "\n",
    "+ **When a predictor is categorical** we can decide to split it to create either one child node per class (*multiway splits*) or only two child nodes (*binary split*). It is usual to make only binary splits because multiway splits break the data into small subsets too quickly. This causes a bias towards splitting predictors with many classes since they are more likely to produce relatively pure child nodes, which results in overfitting.\n",
    "\n",
    "    If a categorical predictor has only two classes, there is only one possible split. However, if a categorical predictor has more than two classes, various conditions can apply. If there is a small number of classes, all possible splits into two child nodes can be considered. For example, for classes apple, banana and orange the three splits are as shown below.\n",
    "    |         | child 1 |     child 2    |\n",
    "    |:-------:|:-------:|:--------------:|\n",
    "    | split 1 |  apple  | banana, orange |\n",
    "    | split 2 |  banana |  apple, orange |\n",
    "    | split 3 |  orange |  apple, banana |\n",
    "    \n",
    "    For k classes there are $2^{k – 1} – 1$ splits, which is computationally prohibitive if k is a large number.    \n",
    "    \n",
    "    If there are many classes, they may be ordered according to their average output value. We can the make a binary split into two groups of the ordered classes. This means there are k – 1 possible splits for k classes. If k is large, there are more splits to consider. As a result, there is a greater chance that a certain split will create a significant improvement, and is therefore best. This causes trees to be biased towards splitting variables with many classes over those with fewer classes.\n",
    "\n",
    "\n",
    "###  Criteria used to imporove the split\n",
    "\n",
    "The type of criteria used while splittig the node depends on our target variable wether it is continous or categorical in nature.\n",
    "\n",
    "**Continuous Target Variable**\n",
    "+ **Reduction in sum of Squared Errors (SSE)**\n",
    "\n",
    "When the outcome is numeric, the relevant improvement is the difference in the sum of squared errors between the node and its child nodes after the split. For any node, the squared error is:\n",
    "\n",
    "  $$ \\sum_{i=1}^{n}{(y_i-c)}^2 $$\n",
    "  \n",
    "where n is the number of cases at that node, c is the average outcome of all cases at that node, and yi is the outcome value of the ith case. If all the yi are close to c, then the error is low. A good clean split will create two nodes which both have all case outcomes close to the average outcome of all cases at that node.\n",
    "\n",
    "\n",
    "**Categorical Target Variable**\n",
    "+ **Gini Impurity**  \n",
    "\n",
    "    When the outcome is categorical, the split may be based on either the improvement of Gini impurity or cross-entropy:\n",
    "\n",
    "  $$ Gini\\ impurity=\\sum_{i=1}^{k}{p_i(1-p_i)}$$ \n",
    "  \n",
    "    where $k$ is the number of classes and $p_i$ is the proportion of cases belonging to class $i$. These two measures give similar results and are minimal when the probability of class membership is close to zero or one. The goal is to split with the largest reduction in weighted gini impurity from child nodes from gini impurity of parent node.\n",
    "\n",
    "\n",
    "+ **Information Gain** \n",
    "\n",
    "    Entropy : In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. It is calculated by the formula.\n",
    "    $$Cross\\ entropy=-\\sum_{i=1}^{k}{p_ilog_2(p_i)} $$\n",
    "    \n",
    "    If the entropy is high it is more difficult to predict the outcomes and vice versa. Consider an example if flipping a coin. It has 2 outcomes heads and tails. Now lets calcualte the entropy.\n",
    "    \n",
    "    probability of head or tail  $1/2$\n",
    "    \n",
    "    $$H = - \\bigg ( p_{head}(log_2\\ p_{head}) + p_{tail}(log_2\\ p_{tail})\\bigg) = - \\bigg ( \\frac{1}{2}(log_2\\ \\frac{1}{2}) + \\frac{1}{2}(log_2\\ \\frac{1}{2})\\bigg) =  -2 *\\bigg ( \\frac{1}{2}\\bigg )log_2\\ \\frac{1}{2} = 1 $$\n",
    "    \n",
    "    If we consider rolla a die then the probabilty of getting any particular outcome from 1 to 6 is $1/6$. So the entopy will be\n",
    "    \n",
    "    $$ H = -6 *\\bigg ( \\frac{1}{6}\\bigg )log_2\\ \\frac{1}{6} = 2.58 $$\n",
    "    \n",
    "    We notice that the entropy of rolling a die is higher so it can be said that predicting the outcoem of dice is moch difficult than of the outcome of predicting a coin toss.\n",
    "    \n",
    "    When multiple variables are involved one way to understand the importance of variable in predicting results is by estimating *Information gain* using that variable. Information gain is calculated by diffrence in entropy before split and after split. \n",
    "    \n",
    "    $$ Information\\ Gain = Entropy(Parent) - Entropy(Children) $$\n",
    "    \n",
    "    we will look more on how this is calculated in the coming sections in detail.\n",
    "    \n",
    "    \n",
    "+ **Chi-Square**\n",
    "\n",
    "    Chi-square is another method of splitting nodes in a decision tree for datasets having categorical target values. It can make two or more than two splits. It works on the statistical significance of differences between the parent node and child nodes.\n",
    "\n",
    "    Chi-Square value is:\n",
    "\n",
    "    $$Chi-Square = \\sqrt{\\frac{(Actual-Expected)^2}{Expected}}$$\n",
    "\n",
    "    Here, the Expected is the expected value for a class in a child node based on the distribution of classes in the parent node, and Actual is the actual value for a class in a child node.\n",
    "\n",
    "    The above formula gives us the value of Chi-Square for a class. Take the sum of Chi-Square values for all the classes in a node to calculate the Chi-Square for that node. Higher the value, higher will be the differences between parent and child nodes, i.e., higher will be the homogeneity.\n",
    "\n",
    "\n",
    "### Illustration of node splitting considering Information gain for a Decision Tree\n",
    "\n",
    "Consider an Experiment in which we have two predictors *variable1* and *variable2* and Target has two outcomes *stop* and *continue*.\n",
    "\n",
    "| variable1 | variable2 |  outcome |\n",
    "|:---------:|:---------:|:--------:|\n",
    "|     3     |     5     |   stop   |\n",
    "|     7     |     6     | continue |\n",
    "|     3     |     3     |   stop   |\n",
    "|     4     |     8     | continue |\n",
    "|     3     |     9     | continue |\n",
    "|     6     |     5     |   stop   |\n",
    "|     5     |     8     | continue |\n",
    "|     6     |     4     | continue |\n",
    "\n",
    "Now lets calculate the intial entropy of root node before any spliting\n",
    "\n",
    "$$H = -\\bigg (\\frac{3}{8}\\bigg )log_2\\frac{3}{8} - \\bigg (\\frac{5}{8}\\bigg )log_2\\frac{5}{8} = 0.954$$\n",
    "\n",
    "Now we have two options either to choose variable1 for split or variable2. To decide on which variable to splot on  we will first calculate entropies in both the cases\n",
    "\n",
    "If we choose *variable1* for split on split point 4 (which is average or 50 precentile) then we calulate *weighted entropy* after the split as follows\n",
    "\n",
    "$p_{(>4)} = 4/8$\n",
    "\n",
    "\n",
    "$H_{(>4)} = -\\frac{1}{4}log_2\\frac{1}{4} - \\frac{3}{4}log_2\\frac{3}{4} = 0.81$\n",
    "\n",
    "| variable1 | outcome |\n",
    "|:---------:|:--------:|\n",
    "|     7     | continue |\n",
    "|     6     |   stop   |\n",
    "|     5     | continue |\n",
    "|     6     | continue |\n",
    "\n",
    "$p_{(<=4)} = 4/8$\n",
    "\n",
    "\n",
    "$H_{(<=4)} = -\\frac{2}{4}log_2\\frac{2}{4} - \\frac{2}{4}log_2\\frac{2}{4} = 1.0$\n",
    "\n",
    "| variable1 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     3     |   stop   |\n",
    "|     3     |   stop   |\n",
    "|     4     | continue |\n",
    "|     3     | continue |\n",
    "\n",
    "Entropy after splitting by *variable1* is $H(variable1) = -p_{(>4)}H_{(>4)} - p_{(<=4)}H_{(<=4)} = 0.9$\n",
    "\n",
    "we choose *variable2* for split on split point 6 (which is average or 50 precentile) then we calulate *weighted entropy* after the split as follows\n",
    "\n",
    "$p_{(>6)} = 3/8$\n",
    "\n",
    "$H_{(>6)} = -\\frac{3}{3}log_2\\frac{3}{3}-\\frac{0}{3}log_2\\frac{0}{3} = 0$\n",
    "\n",
    "| variable2 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     8     | continue |\n",
    "|     9     | continue |\n",
    "|     8     | continue |\n",
    "\n",
    "\n",
    "$p_{(<=6)} = 3/8$\n",
    "\n",
    "$H_{(<=6)} = -\\frac{2}{5}log_2\\frac{2}{5}-\\frac{3}{5}log_2\\frac{3}{5} = 0.97$\n",
    "\n",
    "| variable2 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     5     |   stop   |\n",
    "|     6     | continue |\n",
    "|     3     |   stop   |\n",
    "|     5     |   stop   |\n",
    "|     4     | continue |\n",
    "\n",
    "Entropy after splitting by *variable2* is $H(variable2) = - p_{(>6)}H_{(>6)} - p_{(<=6)}H_{(<=6)} = 0.28$\n",
    "\n",
    "Now let us calculate information gain after each split considering *variable1* and *variable2*.\n",
    "\n",
    "$$IG(1) = H - H(variable1) = 0.954 - 0.9 = 0.054$$\n",
    "\n",
    "$$IG(2) = H - H(variable2) = 0.954 - 0.28 = 0.674$$\n",
    "\n",
    "We see that if we split the node by consedering *variable2* we have *highest information gain*. So we select *variable2* for the split and then we continue repeating the process for impure nodes till we obtain pure leaf nodes for the decision tree. Following diagram shows the decision tree in this case.\n",
    "\n",
    "![](./fig/DecisionTree.png)\n",
    "\n",
    "### Illustration of node splitting considering Gini Impurity for a Decision Tree\n",
    "\n",
    "Consider the same Experiment data from above.\n",
    "\n",
    "| variable1 | variable2 |  outcome |\n",
    "|:---------:|:---------:|:--------:|\n",
    "|     3     |     5     |   stop   |\n",
    "|     7     |     6     | continue |\n",
    "|     3     |     3     |   stop   |\n",
    "|     4     |     8     | continue |\n",
    "|     3     |     9     | continue |\n",
    "|     6     |     5     |   stop   |\n",
    "|     5     |     8     | continue |\n",
    "|     6     |     4     | continue |\n",
    "\n",
    "We have two variables for splitting. we have to calculate weighted gini impurity for both vairable1 and variable2. Whichever variable gives the lowest impurity we will select that variable for split.\n",
    "\n",
    "Lets take the split point as 4 (average value) and calcuate the impurity for variable1\n",
    "\n",
    "$$Gini Impurity (>4) = 1 - \\bigg(\\bigg(\\frac{3}{4}\\bigg)^2 + (\\bigg(\\frac{1}{4}\\bigg)^2\\bigg) = 0.375$$\n",
    "\n",
    "| variable1 | outcome |\n",
    "|:---------:|:--------:|\n",
    "|     7     | continue |\n",
    "|     6     |   stop   |\n",
    "|     5     | continue |\n",
    "|     6     | continue |\n",
    "\n",
    "$$Gini Impurity (<=4) = 1 - \\bigg(\\bigg(\\frac{2}{4}\\bigg)^2 + (\\bigg(\\frac{2}{4}\\bigg)^2\\bigg) = 0.375$$\n",
    "\n",
    "| variable1 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     3     |   stop   |\n",
    "|     3     |   stop   |\n",
    "|     4     | continue |\n",
    "|     3     | continue |\n",
    "\n",
    "$$Weighted Gini Impurity(variable1) = \\frac{4}{8}*0.375 +  \\frac{4}{8}*0.5 = 0.4375$$\n",
    "\n",
    "Lets take the split point as 6 (average value) and calcuate the impurity for variable2\n",
    "\n",
    "$$Gini Impurity (>6) = 1 - \\bigg(\\bigg(\\frac{3}{3}\\bigg)^2 + (\\bigg(\\frac{0}{3}\\bigg)^2\\bigg) = 0$$\n",
    "\n",
    "| variable2 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     8     | continue |\n",
    "|     9     | continue |\n",
    "|     8     | continue |\n",
    "\n",
    "$$Gini Impurity (<=6) = 1 - \\bigg(\\bigg(\\frac{2}{5}\\bigg)^2 + (\\bigg(\\frac{3}{5}\\bigg)^2\\bigg) = 0.48$$\n",
    "\n",
    "| variable2 |  outcome |\n",
    "|:---------:|:--------:|\n",
    "|     5     |   stop   |\n",
    "|     6     | continue |\n",
    "|     3     |   stop   |\n",
    "|     5     |   stop   |\n",
    "|     4     | continue |\n",
    "\n",
    "$$Weighted Gini Impurity(variable2) = \\frac{3}{8}*0 +  \\frac{5}{8}*0.48= 0.3$$\n",
    "\n",
    "From the above calculations we see that Weighted gini impurity is less for variable2. So we choose variable 2 for splitting the data. This process is continued recursively to build the entire decision tree.\n",
    "\n",
    "**Note**: For same data using gini impurity and entropy measure, different variables can be selected as root node, THis is because of bias of each algorithm. The gini impurity is biased toawards selecting wider spread of data for variable selection, whereas the entopy method is biased toward compact data with lower spread.\n",
    "\n",
    "### Advantages of Decision Tree\n",
    "\n",
    "+ It can be used for both classification and regression problems.\n",
    "+ It can handle both continuous and categorical variables.\n",
    "+ No feature scaling (standardization and normalization) required in case of decision tree as it uses rule-based approach instead of distance calculation.\n",
    "+ Non linear parameters don't affect the performance of a decision tree unlike the curve-based algorithms. So, there is high non-linearity between the independent variables, Decision tree may out form as compared to the other curve-based algorithms. \n",
    "+ Decision tree can automatically handle missing values.\n",
    "+ Decision tree is usually robust to outlier and can handle them automatically.\n",
    "+ Training period is less as compared to Random Forest because it generates only one tree unlike forest of trees in the Random Forest.\n",
    "\n",
    "### Disadvantages of Decision Tree\n",
    "\n",
    "+ Overfitting is the main problem of decision tree. It generally leads to overfitting of the data which ultimately leads to wrong prediction.\n",
    "+ Due to the overfitting, there are very high chances of high variance in the output which leads to many errors in the final estimation and shows high in accuracy in the results.\n",
    "+ Adding a new datapoint can lead to re-generation of the overall tree and all nodes need to be recalculated and recreated.\n",
    "+ Decision tree is not suitable for large datasets. If the dataset is large, then one single tree may grow complex and lead to overfit. So, in this case, we should use random forest instead of a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e5fcb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec683d22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fbcb551",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "Some of the popular algorithms used for constructing decision trees are:\n",
    "\n",
    "1. ID3 (Iterative Dichotomiser): Uses Information Gain as attribute selection measure.\n",
    "\n",
    "2. C4.5 (Successor of ID3):  Uses Gain Ratio as attribute selection measure.\n",
    "\n",
    "3. CART (Classification and Regression Trees) – Uses Gini Index as attribute selection measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b1ca4",
   "metadata": {},
   "source": [
    "The above diagram shows how the concept of Pruning works.\n",
    "There are basically two types of Pruning:\n",
    "Pre-Pruning\n",
    "Post Pruning\n",
    "In Pre-pruning, we set parameters like ‘min_samples’, ‘max_depth’, and ‘max_leaves’ during the creation of the tree. All of the parameters need hyperparameter tuning and found using cross-validation and grid-search methods. It restricts the tree to a certain depth or a certain number of leaves.\n",
    "Post-pruning methods are mostly done after the tree is already formed. Cost complexity pruning is the most used post-pruning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec3186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d04392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5326f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
