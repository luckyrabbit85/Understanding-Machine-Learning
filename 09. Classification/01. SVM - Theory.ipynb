{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76030f2c",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "+ Effective in high dimensional spaces.\n",
    "+ Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "+ Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "+ Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "+ If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "+ SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities)\n",
    "\n",
    "### Why do we need SVC when we have Logistic Regression?\n",
    "\n",
    "Logistic regression for classification separates the instances into two classes. However, there is an infinite number of decision boundaries, Logistic Regression only picks an arbitrary one. Logistic Regression doesn't care whether the instances are close to the decision boundary. Therefore, the decision boundary it picks may not be optimal which makes us less confident in our predictions. Therefore, the optimal decision boundary should be able to maximize the distance between the decision boundary and all instances. i.e., maximize the margins. That's why SVMs algorithm is important!\n",
    "\n",
    "![](./fig/svm.png)\n",
    "\n",
    "### Support Vector, Hyperplane and Margin\n",
    "\n",
    "The vector points closest to the hyperplane are known as the support vector points because only these two points are contributing to the result of the algorithm, other points are not. If a data point is not a support vector, removing it has no effect on the model. On the other hands, deleting the support vectors will then change the position of the hyperplane.\n",
    "\n",
    "The dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.\n",
    "\n",
    "The distance of the vectors from the hyperplane is called the margin which is a separation of a line to the closest class points. We would like to choose a hyperplane that maximises the margin between classes. \n",
    "\n",
    "When all support vectors have same distance form maximum margin hyperplane then its called a Good Margin, in other casese it called Bad Margin.\n",
    "\n",
    "![](./fig/margin.png)\n",
    "\n",
    "The maximum margin classifier is super sensitive for outliers in data. Choosing a threshold that allows misclassification is an example of Bias-Variance Tradeoff. Thats why we have soft margins. To compare which softmargin is better we use cross validation where in we observe that by allowing a misclassfied observation inside our margin how well the classifier does on validation or test data. Soft margin is allowed by a tuning parameter called simply C that defined the magnitude of the wiggle allowed across all dimensions. The C parameter defines the amount of violation of the margin allowed. If C = 0 , then there is no violation and we have simple Maximal-Margin Classifier! The larger the value of C, the more violations of hyperplane are permitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97599587",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We have seen support vector classifier for data which is linearly seperable. But what if our data is not linearly seperable. What do we do, This is the time to Introduce **Support vector Machines**. Suppose our data looks like this.\n",
    "\n",
    "![](./fig/nl.png)\n",
    "\n",
    "\n",
    "### Calculating High dimension relationships\n",
    "\n",
    "Suppose we have a y axis with a value dosage square, Then if we plot the data tha data would look like in the figure below and we can see we can have a classifier to seperate our data.\n",
    "\n",
    "![](./fig/hd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdab45d",
   "metadata": {},
   "source": [
    "Now we might have a question, why choose dosage square? why not cube or any other function. Support Vector machines uses something called **Kernel** inorder to find support vector classifier in **higher dimensions**. In the example above we used a **polynomial kernel** which has a parameter $d$ which stands for degree of polynomial. When d = 1 the polynomial kernel computes the relationship between each data points in one-dimension and these relationships are used to find a support vector classifier. When d = 2 we get a second dimension relationships between each pair of observations, and those relationships are used to find a support vecor classifier in two dimension. Polynomial kernel systematically increases $d$ to find a support vector classifier in each dimension. We can find a good value of $d$ using cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1cbd8",
   "metadata": {},
   "source": [
    "Another commonly used kernal is **Radial Kernel**  also known as **RBF kernel (Radial Bias Function)** RBF kernel finds support vector classifier in infinite dimensions so its hard for us to visualize. However while classifying a new observation it behaves like a weighted nearest neighbour mode. In other words the closest observation have more infulence on classifacation when compared to those observations which are far aawy form the data point.\n",
    "\n",
    "Note: Kernal functions only calculate the relationships between data points as if they arein higher dimensions. They do not actually do the transformation. This trick of calculating the high dimentional relationships between the data points, without actually transforming the data is also reffered as **Kernel Trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4de07",
   "metadata": {},
   "source": [
    "**Polynomial Kernel**\n",
    "\n",
    "Polynomial kernel  = $(a*b+r)^d$, where $a$ and  $b$ refer to two different observations in the data set. $r$ determines the coefficient of the polynomial. $d$ sets the degree of the polynomial. Lets set r = 1/2 and d = 2.\n",
    "\n",
    "$(a*b+1/2)^2 = (a*b+1/2)(a*b+1/2) = ab + a^2b^2 + 1/4 = (a, a^2, 1/2).(b. b^2, 1/2)$\n",
    "\n",
    "Now we have dot product of two vectors. The first term refers to x coordinates and second refers to y coordiantes and third refers to z coordinates. Alternatively if we look at\n",
    "\n",
    "$(a*b+1)^2 = (a*b+1)(a*b+1) = 2ab + a^2b^2 + 1 = (\\sqrt{2}a, a^2, 1/2).(\\sqrt{2}b, b^2, 1)$\n",
    "\n",
    "The new x axis coordiantes are moved by a factor of $\\sqrt{2}$ and y axis coordiantes are squared. We can ignore the z azis coordiante in both of the above cases since they are constant values. We see to calculate the high dimensional relationship all we have to do is drot product of those two points. We need not transform the data. Just plug in the values and we get the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e0643",
   "metadata": {},
   "source": [
    "**Radial Kernel**\n",
    "\n",
    "radial Kernel = $e^{-\\gamma(a-b)^2}$ a and b refers to 2 different observations in the data set. $(a-b)^2$ is squared distance between the observations. $\\gamma$ which is determined by cross validation scales the squared distance, thus scaling the infuence. It is dot product of two infinite dimension vectors. This can be further studied using taylor series expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092b5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
