{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99ad99e",
   "metadata": {},
   "source": [
    "### Regression Analysis:\n",
    "\n",
    "Linear regression is where we try to find best fit line between independednt and dependent variable.\n",
    "\n",
    "$$y = b_0 + m_1x_1$$\n",
    "\n",
    "*y* is our dependent variable and *x* is our independent variable.  \n",
    "*m* is the slope and *b* is the bias.\n",
    "\n",
    "### How it works?\n",
    "\n",
    "We fit a line across our points and then calcualte the error (i.e difference between predicted value and original value) and our goal is to minimize this error by adjusting our learning parameters in this case the slope(m) and bias(b).\n",
    "\n",
    "Multivariable regression looks something like this $y = b_0 + m_1x_1 + m_2x_2 + m_3x_3 + ... m_nx_n$\n",
    "\n",
    "\n",
    "### Assumptions in Regression Analysis:\n",
    "\n",
    "1. There should be linear relationship between independent and dependent variable.\n",
    "    + pair plot can be used to check this.\n",
    "    \n",
    "    \n",
    "2. The independent variable should not be correlated - no multicollinearity  \n",
    "    + scatter plot to visualize correlation effect among variables\n",
    "    + VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity.\n",
    "    \n",
    "    \n",
    "3. The error terms must have constant variance - homoskedasticity\n",
    "    + plot error and predicted value. It should not be funnel shaped. It should be uniform.\n",
    "    \n",
    "    \n",
    "4. There should be no correlation between the residual (error) terms - no Autocorrelation.\n",
    "    + residual vs fitted values plot.\n",
    "    \n",
    "    \n",
    "5. The error terms must be normally distributed.\n",
    "    + Q-Q plot should be linear if normally distributed\n",
    "\n",
    "\n",
    "\n",
    "### Regression Analysis consists of 3 stages:\n",
    "\n",
    "1. Analyzing the correlation and directionality of the data    \n",
    "2. Estimating the model, i.e., fitting the line, and \n",
    "3. Evaluating the validity and usefulness of the model.\n",
    "\n",
    "\n",
    "### Covariance  and corelation:\n",
    "\n",
    "Covariance signifies the direction of the linear relationship between the two variables. By direction we mean if the variables are directly proportional or inversely proportional to each other.\n",
    "\n",
    "$$Cov(x,y) =\\frac{\\sum (x-\\overline x)(y-\\overline y)}{N}$$\n",
    "\n",
    "Correlation analysis is a method of statistical evaluation used to study the strength of a relationship between two, numerically measured, continuous variables. It not only shows the kind of relation (in terms of direction) but also how strong the relationship is.\n",
    "\n",
    "$$ Corr = \\frac{Cov(x,y)}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "The correlation coefficient is a dimensionless metric and its value ranges from -1 to +1\n",
    "\n",
    "### Normal method for finding parameters:\n",
    "\n",
    "If we consider $\\theta_0 , \\theta_1,..\\theta_n$ as weight parameters and $x_0, x_1, x_2,.. x_n$ as independent variables. (Note: $\\theta_0$ can be considered as bias and $x_0$ = 1)\n",
    "\n",
    "$$Y = \\theta * X$$\n",
    "\n",
    "Then the loss function: $$J(\\theta) = (Y-X\\theta)^T(Y-X\\theta)$$\n",
    "\n",
    "$$\\theta = (X^TX)^{-1}X^TY$$\n",
    "\n",
    "Sometimes, $X^TX$ may not be invertible, or, since matrix inversion is a compute intensive operation, sometimes, it might be computationally intensive to invert $X^TX$. In such cases, we use another method called Gradient Descent\n",
    "\n",
    "### Loss functions:\n",
    "\n",
    "$$Mean\\ Squared\\ Error\\ or\\ L1\\ loss =\\frac{\\sum_{i=1}^n(y_i - y_i^p)^2}{n} $$\n",
    "\n",
    "$$Mean\\ Absolute\\ Error\\ or\\ L2\\ Loss = \\frac{\\sum_{i=1}^n|y_i - y_i^p|}{n}$$\n",
    "\n",
    " $$ Huber\\ Loss\\ L_{\\delta}(y, f(x)) = \n",
    " \\begin{cases} \n",
    " \\frac{1}{2} * (y - f(x))^2, \\quad |y - f(x)| \\leq \\delta \\\\ \n",
    " \\delta * (|y - f(x)| - \\frac{1}{2} * \\delta),   \\quad otherwise \n",
    " \\end{cases}$$\n",
    " \n",
    "MSE is sensitive to outliers as the error term get squared and becomes too larage.  \n",
    "MAE is robust to Outliers.\n",
    "\n",
    "\n",
    "### Goodness of fit:\n",
    "The definition of R-squared is fairly straight-forward, it is the percentage of the response variable variation that is explained by a linear model. It is also called as coefficient of determination.\n",
    "\n",
    "\n",
    "R-squared = Explained variation / Total variation\n",
    "\n",
    "$$ R-squared\\ = 1 - \\frac{\\sum(y_i - y_i^p)^2}{\\sum(y_i - \\overline y)^2}$$\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "+ 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "+ 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "Adding more independent variables or predictors to a regression model tends to increase the R-squared value, which tempts makers of the model to add even more variables. This is called overfitting and can return an unwarranted high R-squared value. Adjusted R-squared is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables.\n",
    "\n",
    "$$Adjusted R- Squared = 1 - \\frac{(1-R^2)(N-1)}{N-p-1}$$\n",
    "\n",
    "$N$ is samplesize, $p$ is number of predictors and $R^2$ is sample R squared\n",
    "\n",
    "### Regularization Techniques:\n",
    "\n",
    "This technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
    "\n",
    "$$L_{lasso} = \\frac{1}{n}\\sum_{i=1}^n\\bigg(y_i - \\sum_{j=1}^p\\theta_jx_{ij}\\bigg)^2 + \\lambda\\sum_{i=1}^p|\\theta_j|$$\n",
    "\n",
    "\n",
    "$$L_{ridge} = \\frac{1}{n}\\sum_{i=1}^n\\bigg(y_i - \\sum_{j=1}^p\\theta_jx_{ij}\\bigg)^2 + \\lambda\\sum_{i=1}^p\\theta_j^2$$\n",
    "\n",
    "$\\lambda$ is the tuning parameter that decides how much we want to penalize the flexibility of our model.\n",
    "\n",
    "Another technique is to use validation set and train set to avoid overfitting. K- Fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a40a00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
