{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d04ecb3",
   "metadata": {},
   "source": [
    "# Gradient Descent and its Variations\n",
    "\n",
    "Gradient Descent is an optimization algorithm used for minimizing the cost function in various machine learning algorithms. It is basically used for updating the parameters of the learning model.\n",
    "\n",
    "There are mailnly three types of gradient Descent:\n",
    "\n",
    "**Batch Gradient Descent**: This is a type of gradient descent which processes ***all the training samples*** for each iteration and then updates the weights. But if the number of training samples are large, then batch gradient descent is computationally very expensive. Hence for large numebr of training samples batch gradient descent is not preferred. Instead, we prefer to use stochastic gradient descent or mini-batch gradient descent.\n",
    "\n",
    "The way batch gradient descent works is, the gradients are accumulated till the whole data set is processed and then is divided by the number of data instances. In this way, we get an averaged gradient across all data instances in the dataset. The weights are now updated in the negative direction of this averaged gradient.\n",
    "\n",
    "This is less computationally demanding since the gradients are not updated frequently and it has ***stable convergence*** which means by averaging the whole data set it can determine the true gradient.\n",
    "\n",
    "Some disadvantages are, it has very slow learning and can get stuck a local minima.\n",
    "\n",
    "**Stochastic Gradient Descent**: This is a type of gradient descent which processes **one training sample** per iteration and then updates the weights. Hence, the parameters are being updated even after one iteration in which only a single sample has been processed. Hence this is quite faster than batch gradient descent. But again, when the number of training samples is large, even then it processes only one example which can be additional overhead for the system as the number of iterations will be quite large and it has to update weights after every iteration.\n",
    "\n",
    "With stochastic gradient descent we have difficulties to settle on a global minimum, but usually, don't get stuck in local minima and also the gradients can be very noisy since it changes for every single sample.\n",
    "\n",
    "**Mini Batch gradient descent**: This is a type of gradient descent which works faster than both batch gradient descent and stochastic gradient descent. Here a small number of samples from the whole training set are processed per iteration and then the weights are updated. So even if the number of training examples is large, it is processed in small batches of training samples in one go. Thus, it works for larger training samples and that too with lesser number of iterations.\n",
    "\n",
    "Suppose If we fix a batch size N. We calculate N gradients for the N data instances in each mini-batch, sum them up, and divide them by N to get the gradient average over that mini-batch. Then we use this average--mini-batch gradient to update the weight parameters. Batch Gradient descent can prevent the noisiness of the gradient, but we can get stuck in local minima and saddle points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaef8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
