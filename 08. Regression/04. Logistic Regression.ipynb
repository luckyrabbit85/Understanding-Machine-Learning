{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80929ae5",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We have learnt how linear regression works for predicting continous values. But what if we want to predict soemthing like 'yes' and 'no' types. Example: Knowing temprature, humidity and wind data can we predict wether it will rain or not? These types of True-False, Yes-No or 1-0 type of questions basically falls under binary classification or logistic regression. The working is expalned below.\n",
    "\n",
    "We calculate $z$ similar to linear regression\n",
    "\n",
    "$$z = w_0 + w_1x_1 + w_2x_2 + .. w_nx_n$$\n",
    "\n",
    "Then we pass, $z$ into logistic function\n",
    "\n",
    "$$ y^p = h(x) = \\frac{1}{1+e^{-z}} = \\frac{1}{1+e^{-(w_0 + w_1x_1 + w_2x_2 + .. w_nx_n)}}$$\n",
    "\n",
    "\n",
    "### Log Loss\n",
    "\n",
    "The cost fuction for logistic regression called as Log Loss and is given by:\n",
    "\n",
    "$$ J(W) = -y*logy^p - (1-y)log(1-y^p)$$\n",
    "\n",
    "where $y$ is y actual and $y^p$ is y predicted.\n",
    "\n",
    "We see if our y actual is 1 and model predicts 1 our cost is zero. On the ohter hand if model predict wrong our cost is very high. Same if our y acutal is 0 and model predicts it as 1 then the cost fuction is high, else it is zero.\n",
    "\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "The model learns the weights using the same concept gradient descent:\n",
    "\n",
    "$$w_i = w_i - \\alpha \\frac{\\partial J(W)}{\\partial w_i}$$\n",
    "\n",
    "where i = 1,2,.. n\n",
    "\n",
    "### Ove-vs-All classification:\n",
    "\n",
    "we can use logistic regression for multi class classification by ove-vs-all Method. We create different binary calssification models and use Log loss and train them. If we have 3 classes we have 3 models and take highest predicted probabily as predicted class.\n",
    "\n",
    "Model-1: GROUP 1 vs EVERYTHING ELSE\n",
    "Model-2: GROUP 2 vs EVERYTHING ELSE\n",
    "Model-3: GROUP 3 vs EVERYTHING ELSE\n",
    "\n",
    "### SoftMax\n",
    "\n",
    "For multi class classification on of the popular approach is to use SoftMax algorithm. In logistic regression we calculated $z$ and then subject this $z$ to a sigmoid operation. In softmax calssification, for each class we determine the probability of the object belonging to each class. Hence instead of all-or-nothing belongingness to a class, this algorithm provides a softer gradual answer.\n",
    "\n",
    "|                | z  | $e^z$ | $\\frac{e^z}{\\sum e^z}$ |\n",
    "|----------------|----|-------|------------------------|\n",
    "| class 1        | 2  | 7.39  | 0.73                   |\n",
    "| class 2        | 1  | 2.72  | 0.27                   |\n",
    "| class 3        | -3 | 0.05  | 0.005                  |\n",
    "| Sum $\\sum e^z$ |    | 10.16 | 1.005                  |\n",
    "\n",
    "\n",
    "### Cross Entropy Loss\n",
    "\n",
    "The loss function in case of Multi Classification is called as Cross-Entropy Loss:\n",
    "\n",
    "$$Cross Entropy Loss = - \\sum ^{M}_{c=1} y_{c} log(y^{P}_{c})$$\n",
    "\n",
    "m is number of classes\n",
    "\n",
    "![](./fig/cross-entropy.png)\n",
    "\n",
    "We will discuss when we use cross entropy loss as cost function in later sections while dealing with mult-class problems. Here you need to know that Log Loss is a variation of cross entropy loss used during binary class predictions.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Regularization is same like linear regression only our cost function has changed\n",
    "\n",
    "$$J(W) = -\\frac{1}{m}\\bigg [ \\sum_{i=1}^{m} y^i log(h(x^i)) + (1-y^i)log(1-h(x^i))\\bigg] + \\frac{\\lambda}{2m}\\sum_{j = 1}^{n}w_{j}^{2}$$\n",
    "\n",
    "We can use diffrent values of $\\lambda$ and get different values of $W$. It is very important to check the effictivness of obtained W on new data. WE can use K-Fold Cross Validation techinique to do so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94ace0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
