{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35490d6a",
   "metadata": {},
   "source": [
    "## Kmeans Algorithm\n",
    "\n",
    "Kmeans algorithm is an iterative algorithm that tries to partition the dataset into K pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster’s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.\n",
    "\n",
    "The way kmeans algorithm works is as follows:\n",
    "\n",
    "+ Specify number of clusters K.\n",
    "+ Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement.\n",
    "+ Compute the sum of the squared distance between data points and all centroids. Assign each data point to the closest cluster (centroid).\n",
    "+ Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster.\n",
    "+ Recompute the distance and assign the data points and Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn’t changing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39f233",
   "metadata": {},
   "source": [
    "### Understanding the Different Evaluation Metrics for Clustering\n",
    "\n",
    "**Inertia**\n",
    "\n",
    "- Inertia is the sum of distances of all the points within a cluster from the centroid of that cluster.  \n",
    "- We calculate this for all the clusters and the final inertial value is the sum of all these distances.   \n",
    "- This distance within the clusters is known as **intracluster distance**. So, inertia gives us the sum of intracluster distances\n",
    "\n",
    "**Dunn Index**\n",
    "\n",
    "The Dunn index also takes into account the distance between two clusters. This distance between the centroids of two different clusters is known as **inter-cluster distance**. Let's look at the formula of the Dunn index:\n",
    "\n",
    "$$Dunn\\ index = \\frac{min(inter-cluster\\ distances)}{max(intra-cluster\\ distances)}$$\n",
    "\n",
    "Dunn index is the ratio of the minimum of inter-cluster distances and maximum of intracluster distances.\n",
    "\n",
    "We want to maximize the Dunn index that means our intra-cluster distance should be low and inter-cluster distance should be more.\n",
    "\n",
    "**Elbow Method**\n",
    "\n",
    "We pick k at the spot where SSE (sum of intra cluster distances) starts to flatten out and forming an elbow. \n",
    "\n",
    "![](./fig/elbowmethod.png)\n",
    "\n",
    "**Silhouette Analysis**  \n",
    "Silhouette Coefficient is a metric used to calculate the goodness of a clustering technique. Its value ranges from [-1, 1]. 1: Means clusters are well apart from each other and clearly distinguished. 0: Means clusters are indifferent, or we can say that the distance between clusters is not significant. -1: Means clusters are assigned in the wrong way.\n",
    "\n",
    "$$Silhouette\\ Score = \\frac{(b-a)}{max(a,b)}$$\n",
    "\n",
    "where,\n",
    "a = average intra-cluster distance i.e the average distance between each point within a cluster.\n",
    "b = average inter-cluster distance i.e the average distance between all clusters.\n",
    "\n",
    "**Important things to note about K means**\n",
    "\n",
    "+ Use K-Means++ to Choose Initial Cluster Centroids for K-Means Clustering. It choses centroids such that each of them are far from each other.\n",
    "+ Kmeans gives more weight to the bigger clusters.\n",
    "+ Kmeans assumes spherical shapes of clusters (with radius equal to the distance between the centroid and the furthest data point) and doesn’t work well when clusters are in different shapes such as elliptical clusters.\n",
    "+ If there is overlapping between clusters, kmeans doesn’t have an intrinsic measure for uncertainty for the examples belong to + the overlapping region in order to determine for which cluster to assign each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45382e",
   "metadata": {},
   "source": [
    "## K Modes\n",
    "\n",
    "KMeans uses mathematical measures (distance) to cluster continuous data. The lesser the distance, the more similar our data points are. Centroids are updated by Means.\n",
    "But for categorical data points, we cannot calculate the distance. So we go for KModes algorithm. It uses the dissimilarities (total mismatches) between the data points. The lesser the dissimilarities the more similar our data points are. It uses Modes instead of means.\n",
    "\n",
    "## K Prototype\n",
    "The K-Prototype is the clustering algorithm which is the combination of K-Means and K-Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b29b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
