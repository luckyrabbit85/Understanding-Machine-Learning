{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21695833",
   "metadata": {},
   "source": [
    "## Handling Large Data for Machine Learning\n",
    "\n",
    "Many times, data scientist or analyst finds difficulty to fit large data (multiple #GB/#TB) into memory and this is a common problem in the data science world.\n",
    "\n",
    "\n",
    "+ **In tensorflow and keras:** The Keras deep learning library offers a feature for progressively loading image files and is called flow_from_directory. We can also convert our annotations in TFrecord format which is widely used while training Deep Learning Model. Pandas library that can load large CSV files in chunks.\n",
    "\n",
    "\n",
    "\n",
    "+ **Dask:** Dask is a parallel computing library, which scales #NumPy, pandas, and scikit module for fast computation and low memory. It uses the fact that a single machine has more than one core, and dask utilizes this fact for parallel computation.\n",
    "\n",
    "\n",
    "\n",
    "+ **A Big Data Platform:** If we are talking about Big data which is stored in chunks across RDBMS then we can use Apache spark to load and Preprocess our data. Spark with the MLLib library. That is, a platform designed for handling very large datasets, that allows you to use data transforms and machine learning algorithms on top of it.\n",
    "\n",
    "\n",
    "\n",
    "+ **Using Fast loading libraries like Vaex:** Vaex is a high-performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. It calculates statistics such as mean, sum, count, standard deviation, etc, on an N-dimensional grid for more than a billion (10^9) samples/rows per second. Visualization is done using histograms, density plots, and 3d volume rendering, allowing interactive exploration of big data. Vaex uses memory mapping, zero memory copy policy, and lazy computations for best performance (no memory wasted).\n",
    "\n",
    "\n",
    "\n",
    "+ **Change the Data Format:** Is your data stored in raw ASCII text, like a CSV file? Perhaps you can speed up data loading and use less memory by using another data format. A good example is a binary format like GRIB, NetCDF, or HDF. There are many command-line tools that you can use to transform one data format into another that do not require the entire dataset to be loaded into memory. Using another format may allow you to store the data in a more compact form that saves memory, such as 2-byte integers, or 4-byte floats.\n",
    "\n",
    "\n",
    "\n",
    "+ **Object Size reduction with correct datatypes:** Generally, the memory usage of the data frame can be reduced by converting them to correct datatypes. Almost all the datasets include object datatype which is generally in string format which is not memory efficient. When you consider the date, categorical features like region, city, place names were in the string which takes more memory so if we convert these to respective data types like DateTime, categorical which makes memory usage reduced by more than 10 times as consumed before.\n",
    "\n",
    "\n",
    "\n",
    "+ **Use a Relational Database:** Relational databases provide a standard way of storing and accessing very large datasets. Internally, the data is stored on a disk can be progressively loaded in batches and can be queried using a standard query language (SQL).\n",
    "\n",
    "\n",
    "\n",
    "+ **Store your files in the Parquet format available in Pandas (Python Data Analysis Library):*** If you have trouble reading large data files in CSV format, the right optimization is to read the CSV file and then store it in Parquet format to speed up future readings.\n",
    "\n",
    "\n",
    "\n",
    "+ **Choose your Machine Learning model carefully:** Indeed, some models are known to scroll better than others on very large volumes of data. For Boosting algorithms, LightGBM is much more efficient than XGBoost for very large volumes of data. All this with similar performance in terms of accuracy.\n",
    "\n",
    "\n",
    "\n",
    "+ **Allocate More #Memory:** Some machine learning tools or libraries may be limited by a default memory configuration. Check if you can re-configure your tool or library to allocate more memory. A good example is Weka, where you can increase the memory as a parameter when starting the application.\n",
    "\n",
    "\n",
    "\n",
    "+ **Work with a Smaller Sample:** Take a random sample of your data, such as the first 1,000 or 100,000 rows. Use this smaller sample to work through your problem before fitting a final model on all of your data (using progressive data loading techniques). I think this is a good practice in general for machine learning to give you quick spot-checks of algorithms and turnaround of results.\n",
    "\n",
    "\n",
    "\n",
    "+ **Use a Computer with More Memory:** Perhaps you can get access to a much larger computer with an order of magnitude more memory. For example, a good option is to rent compute time on a cloud service like Amazon Web Services (#AWS) or #Azure or Google Cloud Platform (GCP) that offers machines with tens of gigabytes of RAM for less than a US dollar per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6c9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
