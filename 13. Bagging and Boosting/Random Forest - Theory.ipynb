{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69321d26",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Random Forests are collection of decision trees. Random Forest classifier are an ensemble classifier using many decision tree models. Random Forests are way of averaging multipledecision trees built from different parts of training set with a goal to reduce overfitting by a single decision tree. The increased performance comes with some loss of interpretebility.\n",
    "\n",
    "### Data Bagging\n",
    "\n",
    "Random forest using bagging to construct multiple decision trees. Bagging or boot stap aggregating is a technique of sampling a subset of training data with replacemnet and constructs the model based on sampled trainig set. It is expected to have 2/3rd of unique samples from the dataset in each subset.\n",
    "\n",
    "Random forests frow many decision tree by sampling from the input dataset for a set of features instead of training data samples. This is called as feature bagging.\n",
    "\n",
    "### Feature Bagging\n",
    "\n",
    "Typically for classification problem with $n$ features $\\sqrt{n}$ features are used in each split. For a regression problems number of features recommended is $n/3$. So for any databag, creae multiple sets of feature bags. use crossvalidation to decide which feature bag is most apt for this specific data bag and use this feature bag for the data to create a Decision tree. SO each different bad will have a different set of features chosen for creating DEcision Tree.\n",
    "\n",
    "### Cross validation in Random forests\n",
    "\n",
    "In random Forest, cross validation is estimated internally during the run because if the bagging procedure. For each bag 1/3 of  data is left out which is used for cross validation to identify most useful feature set combination, among multiple randomly chosen feature bags.\n",
    "\n",
    "The proportion of sample classified incorrectly from crossvalidation set is also referred as out of bag OOOB error estimate. OOB is mean prediction error on each training sample *j*, using only the trees that did not have sample *j* in the boot strap sample.\n",
    "\n",
    "### Final Prediction\n",
    "\n",
    "So at the end of bagggin, we have now say n decision trees , each tree with reduced dataset and reduced feature set. For a new data point, prediction value is aggregated from all the trees. Final result is the average value from all the trees.\n",
    "\n",
    "###  Calculating Proximities to handle Missing Values. \n",
    "\n",
    "Proximity of two samples is true if they follow the same decision path to the outcome in a tree. Proximity count provides a measure of how frequently unique pairs of training samples end in the same terminal node. Proximity values in a tree are\n",
    "computed by incrementing the proximity value for each case in the input set that end in the same terminal node by one. If there are N input cases, then the proximity matrix is of size $N$ X $N$. In a given tree of the forest, this value is either 0 or 1 for a pair of input observations. \n",
    "\n",
    "Proximity values are obtained by averaging over all trees in the forest and normalizing the proximity value by dividing with the number of trees. Proximity values can be used as a distance measure between pairs of inputs. The distance measure intuitively can be treated similar to Mahalanobis distance or Euclidean distance. This measure can be used in clustering and finding outliers.Proximities are useful in computing outliers, *missing data* in the dataset, and computing prototypes. One method to replace missing data is to find all the cases that end in same class as the case with missing data of variable *k* in the input and then find the median value for variable *k* that end in class n. This method provides fairly good approximation for missing data in the dataset.\n",
    "\n",
    "### Outliers\n",
    "\n",
    "Outlier value can be computed by normalizing proximity score across all the samples. Compute raw outlier measure by dividing the total number of samples with average proximity score of an input case. If the proximity score is small, then this value will be high. Compute median of all the raw outlier scores, and normalize the raw outliers based on standard deviation from absolute median and median values. The larger the normalized value, the higher the outlier probability.\n",
    "\n",
    "Generally its a good practice to detect outliers through visualizations of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed19520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
