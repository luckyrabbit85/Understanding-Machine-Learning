{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be653106",
   "metadata": {},
   "source": [
    "# Gradient Boosting - Regression\n",
    "\n",
    "Here we will discuss about Gradient boost machine learning algorithm. Before diving into this you should be familiar with decision trees and Adaboost algorithms. We will be using a small data set here to predict the weights of a person. Lets look at our data\n",
    "\n",
    "![](./fig/gbmr.png)\n",
    "\n",
    "Gradient Boost starts by making a single leaf, instead of tree or stump. This leaf represent intial guess for the weights of all the samples. When trying to predict a continous value like weights the initial guess is average of the values. Then like Adaboost it builds a tree based on the errors made by the previous tree but unlike Adaboost this tree is larger than an stump. Gradient boost still restrics the size of tree. In this simple example we will restrict ourself with building 4 leaves. In general Gradient boost can have anwherer from 8 to 32. Like Adaboost, Gradient Boost builds fixed sized trees based on the previous trees errors but unlike Adaboost, each tree can be larger than a stump. Also like Adaboost Gradient Boost scales the trees by the same amount. Then gradient Boost builds another tree based on the errors made by the previous tree and then it scales the tree. It buids trees until it reaches the number of trees that we asked for it to buid or till the point where the trees stop improving further.\n",
    "\n",
    "First thing is to calculate average weight. in other words we are prediction every ones weight to be same 71.2. Now we calcuate the error and store it in residual column.\n",
    "\n",
    "**Error or Residual = (observed weight - predicted weight)**\n",
    "\n",
    "![](./fig/red1.png)\n",
    "\n",
    "Now we build a tree using height, Favorite color and gender to predict the residuals.\n",
    "\n",
    "![](./fig/tree1.png)\n",
    "\n",
    "We replace the values of samples that end up in same leaves as average value of that leaf. Now that we have got our new tree we can start predicting values by adding the average value and and the value obtained by running the data sample through the tree.\n",
    "\n",
    "CAUTION: If we add Average predicted value and the data obtained from the tree directly. We notice  in most of the cases the new predicted value exactly matches with the observed value. This is of no good, this implies we are overfitting the model. Our model will have low bias and high variance.\n",
    "\n",
    "![](./fig/tree2.png)\n",
    "\n",
    "Gradient boost deals with these kinds of issue by introducing a parameter called learning rate to scale the contribution from new tree. The learning rate is a value between 0 and 1. In this case we are setting a learning rate of 0.1. So lets update The Predicted weight column.\n",
    "\n",
    "**New predictions = Averaged prediction + (0.1) * Averaged Errors from Tree1**\n",
    "\n",
    "![](./fig/newp.png)\n",
    "\n",
    "We see that the new predictions ar still farm from our observed values. But still its better than our first leaf which predicted average values. In other words scaling the tree results in a small step towards right direction in minimizing the error.\n",
    "\n",
    "### Gradient in Gradient Boost\n",
    "\n",
    "The gradient here refers to the gradient of loss function, and it is the target value for each new tree to predict.\n",
    "\n",
    "Suppose we have a true value  $y$  and a predicted value  $\\hat{y}$ . The predicted value is constructed from some existing trees. Then you are trying to construct the next tree which gives a prediction $z$. Then your final prediction will be  $\\hat{y} + z$. The correct choice of  $z$  is  $z = y - \\hat{y}$. Therefore, you are now constructing trees to predict  $y-\\hat{y}$.\n",
    "\n",
    "It turns out this is a special case of gradient boosting when your loss function is $L = \\frac{1}{2}(y-\\hat{y})^2$ , and your prediction target for this new tree is the gradient of this loss function as  $y-\\hat{y} = -\\frac{\\partial L}{\\partial \\hat{y}}$\n",
    "\n",
    "All that is left to do is to construct a tree, so that the output of the tree on each input data is the negative gradient.\n",
    "\n",
    "If we continue like this the emperical evidence shows that taking lot of small steps in the right direction resultsin better predictions with a Testing dataset i.e. lower Variance. So lets build another tree so we can take another small step in the right direction. Lets buid an update table of our new predictions and new residuals.\n",
    "\n",
    "![](./fig/red2.png)\n",
    "\n",
    "Note: When we compare our new residuals with the ones from previous table they seem to be less. This also indicates we are doing good.\n",
    "\n",
    "![](./fig/residuals.png)\n",
    "\n",
    "Now lets build a new tree to predict the new residuals and just like before since multiple samples ended up in same leaf nodes, we replace them with their averages.\n",
    "\n",
    "![](./fig/newavg.png)\n",
    "\n",
    "\n",
    "**New predictions = Averaged prediction + (0.1) * Averaged Errors from Tree1 + (0.1) * Averaged Errors from Tree2**\n",
    "\n",
    "**New predictions = Previous Prediction + (0.1) * Averaged Errors from Tree2**\n",
    "\n",
    "![](./fig/newavg2.png)\n",
    "\n",
    "Now that we have calculated the latest prediction of weights after second tree. Its time to calculate residuals.\n",
    "\n",
    "![](./fig/newres.png)\n",
    "\n",
    "Lets look at thre residuals so far. They are all decreasing slowly after addition of each tree.\n",
    "\n",
    "![](./fig/compareresiduals.png)\n",
    "\n",
    "Now we fit this newly calculated residual to tree3 and repeat the process again.. We keep building trees until we reach the maximum specified number of trees to build or there is no imporovement further.. Once our forest is build we can predict our new data samples. This ends our Gradient Boosting for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1dd12c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
