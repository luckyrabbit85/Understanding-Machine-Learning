{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83252f6e",
   "metadata": {},
   "source": [
    "# XGBoost Regression\n",
    "\n",
    "XGBoost tree is  built for large complex datasets but for understanding we will use a small and and simple data. Following plot shows the drug content and its effectiveness. Intially this prediction can be anything, but by default it is 0.5, regardless of wether you are using XGBoost for regression or Classification. It is made up of several XGBoost Trees. \n",
    "\n",
    "\n",
    "![](./fig/XG.png)\n",
    "\n",
    "Each tree starts with a single leaf which contains all the values. Now we calculate something called similarity score.\n",
    "\n",
    "$$ Similarity Score = \\frac{(Sum\\ of\\ Residuals)^2}{Number\\ of\\ residuals + \\lambda}$$\n",
    "\n",
    "$\\lambda$ is a regularization parameter\n",
    "\n",
    "We will learn about $\\lambda$ in later section, for now we will consider value of $\\lambda$ = 0.\n",
    "\n",
    "Now lets calculate the similarity scores of our root nodes with contains all the residuals. It turn out to be 4. Now lets split the residuals by taking the average of lowest two dosages in this case its (10+20)/ 2 equals 15. After splitting lets calculate the smilarity scores gain. We see if the residuals are similar i.e they are on same side. The smilarity score is more. If they are on both sides they cancel out while adding and similarity score is low. After that we calculate gain\n",
    "\n",
    "$$Gain\\ =\\ Left\\ Similarity\\ +\\ Right\\ Similarity\\ -\\ Root\\ Similarity$$\n",
    "\n",
    "![](./fig/gain1.png)\n",
    "\n",
    "Gain for **dosage < 15** is 120.33. Similarly lets calculate gain by shifting our threshold to **dosage < 22.5** which is average of next two values.\n",
    "\n",
    "![](./fig/gain2.png)\n",
    "\n",
    "Now we see the Gain for **dosage < 22.5** is 4 which is less than the gain we calculated for **dosage < 15**. So **dosage < 15** is better at splitting the residuals into clusters of similar values. Now we shift the threshlod so that its the average of last two observations.\n",
    "\n",
    "![](./fig/gain3.png)\n",
    "\n",
    "We see the gain is still less when compared to gain calculated for **dosage < 15**. So **dosage < 15** is better at splitting observations. We will use this threshold for the first branch in the tree.\n",
    "\n",
    "![](./fig/gain4.png)\n",
    "\n",
    "Gain for dosage < 22.5 is 28.17 now shift out thresold between last two observations and calcualte gain for dosage < 32.\n",
    "![](./fig/gain5.png)\n",
    "\n",
    "We notice Gain for dosage < 32 is much higher than the previous gain obtained for dosage < 22.5. So we will fix this as the second branch of our tree. For case of simplicity Lets limit ourselves to depth of 2 levels and we will not split further. However the default is to allow till depth of 6 levels.\n",
    "\n",
    "**Pruning a Tree - gamma value ($\\gamma$ )**\n",
    "\n",
    "Now Lets talk about how to prone these trees. We prune an XGBBoost tree based on its gain value. We will start by picking a number, XGBoost calls that number $\\gamma$ (gamma). We then calculatt the difference between the Gain associated with the lowest branch in the tree and the value for game. If the difference between gain and gamma is negative we remove that branch, other wise if the difference between gain and gamma is positive we dont remove that branch. After removing a branch we calculate the difference of its parent branch. If gamma is too high all branches will be pruned.Setting $\\gamma$ = 0 does not turn of pruning because gain can be negative.\n",
    "\n",
    "**Lambda value ($\\lambda$)**\n",
    "\n",
    "Lambda is  a Regularization Parameter, which means that it is intended to reduce the prediction's sensitivity to individual observations. While using regularization parameter $\\lambda$ > 0, the similarity scores values reduces than what we got with $\\lambda$ = 0. The amount of decrease is inversely proportional to the number of residuals in the node. The gain values are also smaller.\n",
    "So when $\\lambda$ > 0. its easier to prune leaves because the value for gain are smaller. Lambda prevents overfitting.\n",
    "\n",
    "$$outout\\ values = \\frac{(Sum\\ of\\ Residuals)}{Number\\ of\\ residuals + \\lambda}$$\n",
    "\n",
    "The output value of leaves will also be decreased. Thus $\\lambda$, the Regularization parameter, will reduce the prediction's sensitivity to this individual observation.\n",
    "\n",
    "Now that we have made our first tree. We make predictions and The way we do it is taking the intial prediction and adding the tree predcition multiplied by learning rate. XGBoost calls this learning reat as eta and default value is 0.3 \n",
    "\n",
    "![](./fig/output.png)\n",
    "\n",
    "Now that we have made prediction. we calculate the new residuals and these residuals will be samller than the previous one. We build multpile trees till our residuals become significantly smaller and smaller or we have reached the maximum number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c499416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
