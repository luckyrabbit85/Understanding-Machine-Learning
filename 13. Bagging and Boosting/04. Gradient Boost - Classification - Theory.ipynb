{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f009d9",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classification\n",
    "\n",
    "Now that we have learnt how Gradient Boosting for classification works. Its time to look at Gradient Boosting for classification. We will use the following data to predict wether some one loves troll2 movie or not based on the following features.\n",
    "\n",
    "![](./fig/gbmc.png)\n",
    "\n",
    "Similar to Gradient Boosting for Regression where we predicted intial predictions to be average weights. Here we predict intial *log of the odds* \n",
    "\n",
    "$$Log\\ of\\ the\\ odds = log_e\\bigg(\\frac{p}{1-p}\\bigg) = log_e(4/2) = 0.693 \\approx 0.7$$\n",
    "\n",
    "Next we convert this into probability using logistic function.\n",
    "\n",
    "$$\\frac{e^{log_e(odds)}}{1 + e^{log_e(odds)}} = \\frac{e^{log_e(4/2)}}{1 + e^{log_e(4/2)}} = 0.667 \\approx 0.7$$\n",
    "\n",
    "Since the probabilty of loving Troll 2 is greater then 0.5, we can classify everyone in the Training Datasetas seomone who loves Troll 2.\n",
    "\n",
    "![](./fig/cres.png)\n",
    "\n",
    "\n",
    "**Residuals = Observed - Predicted probabilty** \n",
    "\n",
    "If observed is 'Yes' then the value is 1, and 0 for 'No'.  \n",
    "\n",
    "Just like in case of regression in this simple example we limit our leaves to 3. Now lets fit the residuals.\n",
    "\n",
    "**NOTE** We cannot add these residuals directly since they are in terms of probabilty and predictions are in terms of log of odds. So we need some kind of transformation.\n",
    "\n",
    "$$\\frac{\\sum Residual_i}{\\sum[Previous Probabilty_i * (1 - Previous Probabilty_i)]}$$\n",
    "  \n",
    "  \n",
    "Lets look at the tree and final output values form the leaves.\n",
    "\n",
    "![](./fig/t1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa11bf",
   "metadata": {},
   "source": [
    "Now that we have our new transformed output values from our tree. Lets calculate new log odds and new residuals.\n",
    "\n",
    "![](./fig/newlogodds.png)\n",
    "\n",
    "Now from this new residuals we will build tree2 and get predictions i.e new log odds. we keep repearing this procedure until we reach maximum number of trees specified or there is no improvement in predictions further.  Then we will use this forest for predicting new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f2dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
