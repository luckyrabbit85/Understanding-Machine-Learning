{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a992a32",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d53bb2",
   "metadata": {},
   "source": [
    "In a forest of trees made by AdaBoost the trees are usually just a node and two leaves. This is also known as stump. So its just forest of stumps. Stumps are not great at making accurate predictions as, they use only one variable. In constrast, in a forest of stumps made with adaboost some stumps get more weightage in the final classification than others. In random forest each tree is made independent of others but in a forest made with adaboost the trees are dependent and the order of trees matters because the error that first tree makes infuences how the second tree is made and so on.. \n",
    "\n",
    "Lets look at an example where we have to predict a patient has heart disease or not:\n",
    "\n",
    "![](./fig/main.png)\n",
    "\n",
    "Fist thing we do is consider one variable at a time and create a stump to predict heart disease. Initially we give same weightage to all the samples, in this case which is 1/8. Wondering what are these weights? these weights are use to calculate the total error a model makes which will be explained in the later section.\n",
    "\n",
    "![](./fig/1ada.png)\n",
    "![](./fig/2ada.png)\n",
    "![](./fig/3ada.png)\n",
    "![](./fig/stump1.png)\n",
    "\n",
    "We see that the this stump with Patient weight > 176 has the lowest gini index 0.2 and has done a better job at classification.\n",
    "So this will be the first stump in the forest. Next thing we need to know is how much weightage should be given to this particular stump in forest. To calculate that lets see how many samples it has misclassified. In this case we see it has misclassified a single example.\n",
    "\n",
    "The **Total Error** for a stump is the sum of weights associated with incorrectly classified samples. Thus in this case the total error is $1/8$. Note: Because all the sample weights add up to 1 total error will always be between 0 and 1, 0 for perfect stump and 1 for a horrible stump. We use total error to determine the  how much weightage this stump has in the final classification. The formula to calculate this is\n",
    "\n",
    "$$ weightage = \\frac{1}{2}log\\bigg(\\frac{1-Total Error}{Total Error + \\eta}\\bigg)$$\n",
    "\n",
    "we add $\\eta$ a very small number in the denominator because when Total error is 0 this equation will be invalid. Lets calculate the weightage of first stump by pluging in the values.\n",
    "\n",
    "$$ weightage = \\frac{1}{2}log(7) = 0.97$$\n",
    "\n",
    "So our Weightage for the first stump comes out to be is 0.97\n",
    "\n",
    "![](./fig/error1.png)\n",
    "\n",
    "Now that we know that our stump has made one classification error. we will have to increase the weight of the sample that was misclassified and decrease the weigh of others. To do so we use this formula\n",
    "\n",
    "$$ new\\ sample\\ weight =  sample\\ weight * e^{weightage}$$\n",
    "\n",
    "$$ new\\ sample\\ weight =  \\frac{1}{8} * e^{0.97} = 0.33$$\n",
    "\n",
    "Now lets decrease the sample weights for the correctly classified samples. The formula to do this is\n",
    "\n",
    "$$ new\\ sample\\ weight =  sample\\ weight * e^{-weightage}$$\n",
    "\n",
    "$$ new\\ sample\\ weight =  \\frac{1}{8} * e^{-0.97} = 0.05$$\n",
    "\n",
    "Lets look at the table after the sample weights have been updated from the calculations we have done above.\n",
    "\n",
    "![](./fig/weigh1.png)\n",
    "\n",
    "We will normalize the weights so that they add upto 1\n",
    "\n",
    "![](./fig/norm1.png)\n",
    "\n",
    "Now we can use modified sample weights to make the second stump.\n",
    "\n",
    "![](./fig/updated.png)\n",
    "\n",
    "In theory we can use the sample weights to calculate weighted gini indices to determine which variable should be used to split the next stump.\n",
    "\n",
    "The weighted gini index would put more emphasis in correctly classifying the sample which was misclassified by the last stump, since it has the largest Sample weight.\n",
    "\n",
    "Alternatively, instead of using  a weighted gini index, we can make a new collection of samples that contains duplicate copies of the samples with largest sample weights.\n",
    "\n",
    "![](./fig/number.png)\n",
    "\n",
    "To create a new data sample set of same size, we choose a random number between 0 and 100 and see where the number falls when we use sample weights like a distribution. Then we pick that sample and add it to our new sample dataset. Lastly after creating the new sample data set, we give all of the sample equal sample weight of $1/8$ as before. Now we go back to the beginning and try to find the stump that does the best job in classifying they samples. \n",
    "\n",
    "Note: The new sample dataset will contain multiple copies of the wrongly classified data point by the last stump. So that this time it can put more emphasis on classifying it correctly. This is how the error that first tree makes infuence how the second tree is made. The weightages are assigned to the stumps on how well they were able to classify the data samples correctly.\n",
    "\n",
    "\n",
    "After we have create a forest of stumps how do we classify a new data point.?\n",
    "\n",
    "We pass the new data across all the stumps in the forest, some stumps classify the new data as having *heart disease* and others classifying it as having *no heart disease*. \n",
    "We simply add all the weightages of stumps that classify the new data point as *heart disease* and compare with the sum of weightages of the other stumps which classify it as having *no heart disease*. Ultimately which ever has the larger sum we choose that answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33021b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
