{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d000d37",
   "metadata": {},
   "source": [
    "# Understand the Fundamentals of the K-Nearest Neighbors (KNN) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fadd0",
   "metadata": {},
   "source": [
    "KNN is a supervised learning algorithm used for both regression and calssification. To make  predictions KNN doesnt need a predictive model. In KNN we dont have a learning phase. That's why it is sometimes referred to as **Lazy learning algorithm**. KNN is also a **non-parametric learning algorithm** because it doesn't assume anything about the underlying data.\n",
    "\n",
    "### So how does KNN make a prediction?\n",
    "\n",
    "The KNN algorithm stores the entire data set to make prediction. Since it has no training all the data is used in testing phase. This makes training phase faster but testing phase slower and costlier. It takes in lot of memory for storing the training data and is slow in prediction as it has to scan through all the data points. The algorithm looks for the $K$ instances of the data set that is closest to our observation $x$. Then for these K neighbors, the algorithm will use their output in order to calculate the variable $y$ of the observation that we want to predict.\n",
    "\n",
    "+ If KNN is used for a regression problem, the mean (or median) of the y variables of the K closest observations will be used for predictions.\n",
    "+ If KNN is used for a classification problem, it’s the mode (the value that appears most often) of the variables y of the K closest observations that will be used for predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e70c8a",
   "metadata": {},
   "source": [
    "### Understand the algorithm\n",
    "\n",
    "K-nearest neighbors (KNN) algorithm uses 'feature similarity' to predict the values of new datapoints which further means that the new data point will be assigned a value based on how closely it matches the points in the training set. We can understand its working with the help of following steps −\n",
    "\n",
    "1. First step of KNN, we must load the training as well as test data.\n",
    "2. Next, we need to choose the value of K i.e. the nearest data points. K can be any integer.\n",
    "3. For each point in the test data the algorithm does the folowwing:\n",
    "    + Calculates the distance between test data and each row of training data with the help of any of the method namely: Euclidean, Manhattan or Hamming distance. The most commonly used method to calculate distance is Euclidean.\n",
    "    + Now, based on the distance value,  it sort them in ascending order.\n",
    "    + Next, it will choose the top K rows from the sorted array.\n",
    "    + Now, it will assign a class or value to the test point based on most frequent class or average value of these rows.\n",
    "\n",
    "![](data/KNN.png)\n",
    "\n",
    "In the above picture for the new observation (colored green) will be assigned class 0 label for K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a18110",
   "metadata": {},
   "source": [
    "### How to choose the right value for K?\n",
    "\n",
    "To select the value of K for our data we must run the KNN algorithm multiple times with different valuesof K. \n",
    "Then we chooe the k value which returns minimum errors on our test data set.\n",
    "\n",
    "There are few things to keep in mind while deciding the appropriate K value:\n",
    "\n",
    "+ When we decrease the value of K to 1, our predictions become less stable. If one worng class is near to our observation then we get the prediction wrong.\n",
    "+ When we increase the value of K, our predictions become more and more stable due to majority or average voting. We are therefore more likely to make more precise predictions (up to a certain point). On the other hand, we're starting to see an increasing number of errors. It's at this point that we know that we've pushed the value of K too far.\n",
    "+ In cases where we vote by the majority (for example, by choosing the mode in a classification problem) among the labels, we generally choose an odd number for K This eliminates the situation of tie.\n",
    "\n",
    "We need to choose appropriate value of K in such a way that we avoid underfitting and over fitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa1858a",
   "metadata": {},
   "source": [
    "### Pros and Cons of KNN\n",
    "\n",
    "Pros:\n",
    "+ Simple and easy to interpret  \n",
    "+ Does not make any assumptions about the underlying data  \n",
    "+ Works well for multi class classification and regression problems  \n",
    "\n",
    "Cons:\n",
    "+ Becomes slow as the  data points increase.\n",
    "+ Not memory efficient\n",
    "+ Sensitive to Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e30b7",
   "metadata": {},
   "source": [
    "### Applications of KNN\n",
    "\n",
    "+ Finanace - Generating Credit ratings of customers  \n",
    "+ Healthcare - Classifying a patient having disease or healthy  \n",
    "+ Political Science - Classifying potential voeters   wether they will vote or not\n",
    "+ Handwriting detection - Classifying different styles  of Handwritings\n",
    "+ Image recognition - Classifying different categories of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ff267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
